{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Renato Laffranchi FalcÃ£o\n",
    "\n",
    "# Reinforcement Learning for Quantitative Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = lambda ticker: ticker.split(\".\")[0]\n",
    "\n",
    "tickers = [\"COGN3.SA\", \"ITUB4.SA\", \"PETR4.SA\"]\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "n_years = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{data_folder}/COGN3_2014-05-30_2024-05-26.feather\")\n",
    "print(\"Data dataframe shape:\", data.shape)\n",
    "data_size = data.shape[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "train_size = int(data_size * 0.75)\n",
    "train_data = data.iloc[:train_size]\n",
    "test_data = data.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_capital = 10_000\n",
    "\n",
    "\n",
    "def custom_reward_function(history):\n",
    "    \"\"\"\n",
    "    Custom reward function for the trading environment\n",
    "    :param history: history object: History object of the trading environment\n",
    "    :return: float: Reward value\n",
    "\n",
    "    The reward function follows the formula:\n",
    "    reward = (portfolio_valuation[-1] / portfolio_valuation[-2])^3 / risk_free_rate\n",
    "\n",
    "    # Full history documentation: https://gym-trading-env.readthedocs.io/en/latest/history.html\n",
    "    \"\"\"\n",
    "\n",
    "    return (history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", 0])**3 / 10.40\n",
    "\n",
    "# def custom_reward_function(history):\n",
    "#     return np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2])\n",
    "\n",
    "\n",
    "# def custom_reward_function(history):\n",
    "#     slope = history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2]\n",
    "#     if slope > 0:\n",
    "#         return np.sqrt(3 * history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2])\n",
    "#     else:\n",
    "#         return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 1\n",
    "vec_env = make_vec_env(\n",
    "    \"TradingEnv\",\n",
    "    n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        \"name\": \"AlgoTradingTestingEnv\",\n",
    "        \"df\": test_data,\n",
    "        \"positions\": [-1, 0, 1],\n",
    "        \"trading_fees\": 0.01/100,\n",
    "        \"borrow_interest_rate\": 0.03/100,\n",
    "        \"portfolio_initial_value\": initial_capital,\n",
    "        \"reward_function\": custom_reward_function,\n",
    "        \"windows\": 14,\n",
    "        \"verbose\": 1,\n",
    "        \"render_mode\": \"logs\",\n",
    "    }\n",
    ")\n",
    "\n",
    "vec_env_add_metric = vec_env.env_method(\"get_wrapper_attr\", \"add_metric\")[0]\n",
    "vec_env_add_metric(\"Position Changes\", lambda history : np.sum(np.diff(history[\"position\"]) != 0) )\n",
    "\n",
    "model = PPO.load(\"models_reward_func_01/COGN3_PPO\")\n",
    "\n",
    "output = pd.DataFrame()\n",
    "done = False\n",
    "observation = vec_env.reset()\n",
    "while not done:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "    position_index = model.predict(observation)[0] # At every timestep, pick a position index from your position list (=[-1, 0, 1]) using your trained model\n",
    "    observation, reward, done, info = vec_env.step(position_index)\n",
    "    temp_df = pd.DataFrame(info)\n",
    "    output = pd.concat([output, temp_df], ignore_index=True)\n",
    "output.set_index(\"date\", inplace=True)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"position\"].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
